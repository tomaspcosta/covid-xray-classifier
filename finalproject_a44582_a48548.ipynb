{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\jtoma\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch>=2.0.0 torchvision>=0.15.0 scikit-learn>=1.0 numpy>=1.21 Pillow>=9.0 argparse>=1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m     19\u001b[0m TRAIN_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcovid_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m EVAL_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation_Set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'covid_dataset', 'train')\n",
    "EVAL_DIR = os.path.join(BASE_DIR, 'evaluation_Set')\n",
    "RESULT_FILE = os.path.join(BASE_DIR, 'result.txt')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "\n",
    "\n",
    "# Check if required directories exist\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    raise FileNotFoundError(f\"Training directory not found: {TRAIN_DIR}\")\n",
    "if not os.path.exists(EVAL_DIR):\n",
    "    raise FileNotFoundError(f\"Evaluation directory not found: {EVAL_DIR}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]) \n",
    "])\n",
    "\n",
    "# Load Training Dataset\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=transform)\n",
    "\n",
    "\n",
    "labels = [label for _, label in train_data]\n",
    "\n",
    "#stratified split\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train_data)),\n",
    "    test_size=0.2, \n",
    "    stratify=labels,  \n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "# Subset datasets\n",
    "train_dataset = torch.utils.data.Subset(train_data, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(train_data, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load Evaluation Dataset\n",
    "def load_eval_images(eval_dir):\n",
    "    return [os.path.join(eval_dir, fname) for fname in os.listdir(eval_dir) if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "eval_image_paths = load_eval_images(EVAL_DIR)\n",
    "\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, os.path.basename(img_path)\n",
    "\n",
    "eval_dataset = EvalDataset(eval_image_paths, transform)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Model: ResNet18\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training Function\n",
    "def train_model(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Validation Function\n",
    "def validate_model():\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "    unique, counts = np.unique(val_labels, return_counts=True)\n",
    "    print(\"Class distribution in validation set:\", dict(zip(unique, counts)))\n",
    "\n",
    "    print(\"True labels:\", val_labels)\n",
    "    print(\"Predicted labels:\", val_preds)\n",
    "\n",
    "    # Calculate the basic F1 score\n",
    "    val_f1 = f1_score(val_labels, val_preds)\n",
    "    print(f\"Validation F1 Score (basic): {val_f1 * 100:.4f}%\")\n",
    "\n",
    "    # Weighted F1 score\n",
    "    val_f1_weighted = f1_score(val_labels, val_preds, average='weighted')\n",
    "    print(f\"Validation Weighted F1 Score: {val_f1_weighted:.4f}\")\n",
    "\n",
    "    return val_f1\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    with open(RESULT_FILE, 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for inputs, img_names in eval_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                img_name = img_names[0]  \n",
    "                label = preds.item()\n",
    "                f.write(f\"{img_name:<20}{label}\\n\")\n",
    "\n",
    "    print(f\"Evaluation completed. Results saved to {RESULT_FILE}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"COVID-19 X-Ray Classifier\")\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_model(args.epochs)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    print(\"Validating model...\")\n",
    "    validate_model()\n",
    "\n",
    "    print(\"Evaluating model on unseen data...\")\n",
    "    evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
